---
layout: post
title: Variational Inference - Foundations and Innovations
date: 2018-07-04
categories: [computer science]
tags: [machine learning]

---

## Article Source
* Title: [Variational Inference: Foundations and Innovations](https://www.youtube.com/watch?v=Dv86zdWjJKQ)

---


# Variational Inference: Foundations and Innovations

* David Blei, Columbia University @ Computational Challenges in Machine Learning
* [Talk site](https://www.youtube.com/redirect?v=Dv86zdWjJKQ&event=video_description&redir_token=-ZCMhb_6PbKXEH_4_GDYvEi2-2d8MTUzMDgwMTE5OUAxNTMwNzE0Nzk5&q=https%3A%2F%2Fsimons.berkeley.edu%2Ftalks%2Fdavid-blei-2017-5-1)


## Abstract

One of the core problems of modern statistics and machine learning is to approximate difficult-to-compute probability distributions. This problem is especially important in probabilistic modeling, which frames all inference about unknown quantities as a calculation about a conditional distribution. In this tutorial I review and discuss *variational inference* (VI), a method a that approximates probability distributions through optimization. VI has been used in myriad applications in machine learning and tends to be faster than more traditional methods, such as Markov chain Monte Carlo sampling.
 
This tutorial aims to provide both an introduction to VI, a modern view of the field, and an overview of the role that probabilistic inference plays in many of the central areas of machine learning. First, I will provide a review of variational inference. Second, I describe some of the pivotal tools for VI that have been developed in the last few years, tools like Monte Carlo gradient estimation, black box variational inference, stochastic variational inference, and variational autoencoders. Finally, I discuss some of the unsolved problems in VI and point to promising research directions.

<iframe width="600" height="400" src="https://www.youtube.com/embed/Dv86zdWjJKQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>